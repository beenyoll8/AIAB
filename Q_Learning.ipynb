{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Q_Learning:\n",
    "\n",
    "    ##########\n",
    "\n",
    "    def __init__(self, env, alpha, gamma, epsilon, numberOfEpisodes, numberOfBins, lowerBounds, upperBounds):\n",
    "      \n",
    "        import numpy as np\n",
    "\n",
    "        self.env = env # env - Cart Pole environment\n",
    "        self.alpha = alpha # alpha - step size (learning rate)\n",
    "        self.gamma = gamma # gamma - discount rate\n",
    "        self.epsilon = epsilon # epsilon - parameter for epsilon-greedy approach\n",
    "        self.actionNumber = env.action_space.n\n",
    "        self.numberOfEpisodes = numberOfEpisodes  # numberOfEpisodes - total number of simulation episodes\n",
    "        self.numberOfBins = numberOfBins # numberOfBins - list of 4 integers, number of bins for state discretization\n",
    "        self.lowerBounds = lowerBounds # lowerBounds - list of 4 floats, lower bounds for state discretization\n",
    "        self.upperBounds = upperBounds # upperBounds - list of 4 floats, upper bounds for state discretization\n",
    "         \n",
    "        # List to store the sum of rewards for each episode\n",
    "        self.sumRewardsEpisode = []\n",
    "         \n",
    "        # Initialize Q-value matrix with random values\n",
    "        self.Qmatrix=np.random.uniform(low=0, high=1, size=(numberOfBins[0],numberOfBins[1],numberOfBins[2],numberOfBins[3],self.actionNumber))\n",
    "         \n",
    "    ##########\n",
    "\n",
    "    def returnIndexState(self, state):\n",
    "\n",
    "    # This function converts a continuous state into a discrete index tuple for indexing the Q-value matrix based on the discretization grid\n",
    "    # INPUT: state - list of 4 elements: [cart position, cart velocity, pole angle, pole angular velocity]\n",
    "    # OUTPUT: 4-dimensional tuple representing the indices for the Q-value matrix\n",
    "\n",
    "        # Extract state variables\n",
    "        position = state[0]\n",
    "        velocity = state[1]\n",
    "        angle = state[2]\n",
    "        angularVelocity = state[3]\n",
    "        \n",
    "        # Create discretization bins for each state variable\n",
    "        cartPositionBin = np.linspace(self.lowerBounds[0], self.upperBounds[0], self.numberOfBins[0])\n",
    "        cartVelocityBin = np.linspace(self.lowerBounds[1], self.upperBounds[1], self.numberOfBins[1])\n",
    "        poleAngleBin = np.linspace(self.lowerBounds[2], self.upperBounds[2], self.numberOfBins[2])\n",
    "        poleAngularVelocityBin = np.linspace(self.lowerBounds[3], self.upperBounds[3], self.numberOfBins[3])\n",
    "         \n",
    "        # Digitize each state variable to find the corresponding bin index\n",
    "        indexPosition = np.maximum(np.digitize(position, cartPositionBin) - 1, 0)\n",
    "        indexVelocity = np.maximum(np.digitize(velocity, cartVelocityBin) - 1, 0)\n",
    "        indexAngle = np.maximum(np.digitize(angle, poleAngleBin) - 1, 0)\n",
    "        indexAngularVelocity = np.maximum(np.digitize(angularVelocity, poleAngularVelocityBin) - 1, 0)\n",
    "         \n",
    "        # Return the indices as a tuple\n",
    "        return tuple([indexPosition, indexVelocity, indexAngle, indexAngularVelocity])   \n",
    "\n",
    "    ##########\n",
    "\n",
    "    def selectAction(self, state, index):\n",
    "\n",
    "    # This function selects an action based on the current state using the epsilon-greedy approach\n",
    "    # INPUTS: state - the current state for which to compute the action, index - the current episode index\n",
    "    # OUTPUT: An action selected according to the epsilon-greedy policy\n",
    "\n",
    "        # For the first 500 episodes, select completely random actions to ensure exploration\n",
    "        if index < 500:\n",
    "            return np.random.choice(self.actionNumber)\n",
    "        \n",
    "        # Generate a random number in the interval [0.0, 1.0) for epsilon-greedy selection\n",
    "        randomNumber = np.random.random()\n",
    "        \n",
    "        # After 7000 episodes, gradually decrease the epsilon parameter to reduce exploration\n",
    "        if index > 7000:\n",
    "            self.epsilon *= 0.999\n",
    "        \n",
    "        # If the random number is less than epsilon, select a random action (exploration)\n",
    "        if randomNumber < self.epsilon:\n",
    "            return np.random.choice(self.actionNumber)\n",
    "        \n",
    "        # Otherwise, select the action with the highest Q-value (exploitation)\n",
    "        else:\n",
    "            # Get the discretized index of the current state\n",
    "            stateIndex = self.returnIndexState(state)\n",
    "        \n",
    "            # Find the action(s) with the maximum Q-value for the current state\n",
    "            maxQActions = np.where(self.Qmatrix[stateIndex] == np.max(self.Qmatrix[stateIndex]))[0]\n",
    "        \n",
    "            # Randomly choose among the actions with the highest Q-value\n",
    "            return np.random.choice(maxQActions)\n",
    "     \n",
    "    ##########\n",
    "      \n",
    "    def simulateEpisodes(self):\n",
    "\n",
    "    # function for simulating learning episodes\n",
    "\n",
    "        import numpy as np\n",
    "        \n",
    "        # loop through the episodes\n",
    "        for indexEpisode in range(self.numberOfEpisodes):\n",
    "            \n",
    "            # list that stores rewards per episode - this is necessary for keeping track of convergence \n",
    "            rewardsEpisode = []\n",
    "            \n",
    "            # reset the environment at the beginning of every episode\n",
    "            (stateS, _) = self.env.reset()\n",
    "            stateS = list(stateS)\n",
    "            \n",
    "            print(\"Simulating episode {}\".format(indexEpisode))\n",
    "            \n",
    "            # step from one state to another until a terminal state is reached\n",
    "            terminalState = False\n",
    "            while not terminalState:\n",
    "                # return a discretized index of the state\n",
    "                stateSIndex = self.returnIndexState(stateS)\n",
    "                \n",
    "                # select an action on the basis of the current state, denoted by stateS\n",
    "                actionA = self.selectAction(stateS, indexEpisode)\n",
    "                \n",
    "                # step and return the state, reward, and boolean denoting if the state is a terminal state\n",
    "                # prime means that it's the next state\n",
    "                (stateSprime, reward, terminalState, _, _) = self.env.step(actionA)          \n",
    "                \n",
    "                rewardsEpisode.append(reward)\n",
    "                \n",
    "                stateSprime = list(stateSprime)\n",
    "                \n",
    "                stateSprimeIndex = self.returnIndexState(stateSprime)\n",
    "                \n",
    "                # return the max value\n",
    "                QmaxPrime = np.max(self.Qmatrix[stateSprimeIndex])                                               \n",
    "                                                \n",
    "                if not terminalState:\n",
    "                    # stateS + (actionA,) - to append the tuples\n",
    "                    error = reward + self.gamma * QmaxPrime - self.Qmatrix[stateSIndex + (actionA,)]\n",
    "                    self.Qmatrix[stateSIndex + (actionA,)] = self.Qmatrix[stateSIndex + (actionA,)] + self.alpha * error\n",
    "                else:\n",
    "                    # in the terminal state, Qmatrix[stateSprime, actionAprime] = 0 \n",
    "                    error = reward - self.Qmatrix[stateSIndex + (actionA,)]\n",
    "                    self.Qmatrix[stateSIndex + (actionA,)] = self.Qmatrix[stateSIndex + (actionA,)] + self.alpha * error\n",
    "                \n",
    "                # set the current state to the next state                    \n",
    "                stateS = stateSprime\n",
    "            \n",
    "            print(\"Sum of rewards {}\".format(np.sum(rewardsEpisode)))        \n",
    "            self.sumRewardsEpisode.append(np.sum(rewardsEpisode))\n",
    "         \n",
    "    ##########\n",
    "     \n",
    "    def simulateLearnedStrategy(self):\n",
    "        \n",
    "    # function for simulating the final learned optimal policy\n",
    "    # OUTPUT: env1 - created Cart Pole environment, obtainedRewards - a list of obtained rewards during time steps of a single episode\n",
    "\n",
    "        import gym \n",
    "        import time\n",
    "        \n",
    "        # Create the Cart Pole environment with human render mode\n",
    "        env1 = gym.make('CartPole-v1', render_mode='human')\n",
    "        \n",
    "        # Reset the environment to get the initial state\n",
    "        (currentState, _) = env1.reset()\n",
    "        env1.render()\n",
    "        \n",
    "        # Define the number of time steps\n",
    "        timeSteps = 1000\n",
    "        \n",
    "        # List to store obtained rewards at every time step\n",
    "        obtainedRewards = []\n",
    "        \n",
    "        for timeIndex in range(timeSteps):\n",
    "            print(timeIndex)\n",
    "            \n",
    "            # Select greedy actions\n",
    "            actionInStateS = np.random.choice(np.where(self.Qmatrix[self.returnIndexState(currentState)] == np.max(self.Qmatrix[self.returnIndexState(currentState)]))[0])\n",
    "            \n",
    "            # Take the action and get the next state, reward, and whether the state is terminal\n",
    "            currentState, reward, terminated, truncated, info = env1.step(actionInStateS)\n",
    "            \n",
    "            # Append the reward to the list\n",
    "            obtainedRewards.append(reward)\n",
    "            \n",
    "            # Sleep for a short duration to slow down the simulation for human view\n",
    "            time.sleep(0.05)\n",
    "            \n",
    "            if terminated:\n",
    "                # If the episode ends, sleep for a second and break the loop\n",
    "                time.sleep(1)\n",
    "                break\n",
    "        \n",
    "        return obtainedRewards, env1\n",
    "\n",
    "    ##########\n",
    "\n",
    "    def simulateRandomStrategy(self):\n",
    "    \n",
    "    # function for simulating random actions many times to evaluate the optimal policy and to compare it with a random policy\n",
    "    # OUTPUT: sumRewardsEpisodes - every entry of this list is a sum of rewards obtained by simulating the corresponding episode, env2 - created Cart Pole environment\n",
    "    \n",
    "        import gym \n",
    "        import time\n",
    "        import numpy as np\n",
    "        \n",
    "        # Create the Cart Pole environment\n",
    "        env2 = gym.make('CartPole-v1')\n",
    "        \n",
    "        # Reset the environment to get the initial state\n",
    "        (currentState, _) = env2.reset()\n",
    "        env2.render()\n",
    "        \n",
    "        # Number of simulation episodes\n",
    "        episodeNumber = 100\n",
    "        \n",
    "        # Time steps in every episode\n",
    "        timeSteps = 1000\n",
    "        \n",
    "        # Sum of rewards in each episode\n",
    "        sumRewardsEpisodes = []\n",
    "        \n",
    "        for episodeIndex in range(episodeNumber):\n",
    "            rewardsSingleEpisode = []\n",
    "            \n",
    "            # Reset the environment at the start of each episode\n",
    "            initial_state = env2.reset()\n",
    "            print(episodeIndex)\n",
    "            \n",
    "            for timeIndex in range(timeSteps):\n",
    "                # Select a random action\n",
    "                random_action = env2.action_space.sample()\n",
    "                \n",
    "                # Take the action and get the next state, reward, and whether the state is terminal\n",
    "                observation, reward, terminated, truncated, info = env2.step(random_action)\n",
    "                \n",
    "                # Append the reward to the list\n",
    "                rewardsSingleEpisode.append(reward)\n",
    "                \n",
    "                if terminated:\n",
    "                    break\n",
    "            \n",
    "            # Append the sum of rewards for this episode to the list\n",
    "            sumRewardsEpisodes.append(np.sum(rewardsSingleEpisode))\n",
    "        \n",
    "        return sumRewardsEpisodes, env2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
