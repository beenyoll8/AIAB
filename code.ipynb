{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- Q-Learning Off-Policy Control \n",
    "- Reinforcement Learning Tutorial\n",
    "\n",
    "Author: Aleksandar Haber\n",
    "Date: February 2023\n",
    "\n",
    "- This Python file contains driver code for the Q-Learning algorithm\n",
    "- This Python file imports the class Q_Learning that implements the algorithm \n",
    "- The definition of the class Q_Learning is in the file \"functions.py\"\n",
    "\n",
    "The tutorial webpage explaining the Q-learning algorithm and\n",
    "the developed codes is given below:\n",
    "    \n",
    "https://aleksandarhaber.com/q-learning-in-python-with-tests-in-cart-pole-openai-gym-environment-reinforcement-learning-tutorial/\n",
    "\n",
    "\n",
    "'''\n",
    "# Note: \n",
    "# You can either use gym (not maintained anymore) or gymnasium (maintained version of gym)    \n",
    "    \n",
    "# tested on     \n",
    "# gym==0.26.2\n",
    "# gym-notices==0.0.8\n",
    "\n",
    "#gymnasium==0.27.0\n",
    "#gymnasium-notices==0.0.1\n",
    "\n",
    "# classical gym \n",
    "import gym\n",
    "# instead of gym, import gymnasium \n",
    "# import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# import the class that implements the Q-Learning algorithm\n",
    "from functions import Q_Learning\n",
    "\n",
    "#env=gym.make('CartPole-v1',render_mode='human')\n",
    "env=gym.make('CartPole-v1')\n",
    "(state,_)=env.reset()\n",
    "#env.render()\n",
    "#env.close()\n",
    "\n",
    "# here define the parameters for state discretization\n",
    "upperBounds=env.observation_space.high\n",
    "lowerBounds=env.observation_space.low\n",
    "cartVelocityMin=-3\n",
    "cartVelocityMax=3\n",
    "poleAngleVelocityMin=-10\n",
    "poleAngleVelocityMax=10\n",
    "upperBounds[1]=cartVelocityMax\n",
    "upperBounds[3]=poleAngleVelocityMax\n",
    "lowerBounds[1]=cartVelocityMin\n",
    "lowerBounds[3]=poleAngleVelocityMin\n",
    "\n",
    "numberOfBinsPosition=30\n",
    "numberOfBinsVelocity=30\n",
    "numberOfBinsAngle=30\n",
    "numberOfBinsAngleVelocity=30\n",
    "numberOfBins=[numberOfBinsPosition,numberOfBinsVelocity,numberOfBinsAngle,numberOfBinsAngleVelocity]\n",
    "\n",
    "# define the parameters\n",
    "alpha=0.1\n",
    "gamma=1\n",
    "epsilon=0.2\n",
    "numberEpisodes=15000\n",
    "\n",
    "# create an object\n",
    "Q1=Q_Learning(env,alpha,gamma,epsilon,numberEpisodes,numberOfBins,lowerBounds,upperBounds)\n",
    "# run the Q-Learning algorithm\n",
    "Q1.simulateEpisodes()\n",
    "# simulate the learned strategy\n",
    "(obtainedRewardsOptimal,env1)=Q1.simulateLearnedStrategy()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "# plot the figure and adjust the plot parameters\n",
    "plt.plot(Q1.sumRewardsEpisode,color='blue',linewidth=1)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Sum of Rewards in Episode')\n",
    "plt.yscale('log')\n",
    "plt.savefig('convergence.png')\n",
    "plt.show()\n",
    "\n",
    "# close the environment\n",
    "env1.close()\n",
    "# get the sum of rewards\n",
    "np.sum(obtainedRewardsOptimal)\n",
    "\n",
    "# now simulate a random strategy\n",
    "(obtainedRewardsRandom,env2)=Q1.simulateRandomStrategy()\n",
    "plt.hist(obtainedRewardsRandom)\n",
    "plt.xlabel('Sum of rewards')\n",
    "plt.ylabel('Percentage')\n",
    "plt.savefig('histogram.png')\n",
    "plt.show()\n",
    "\n",
    "# run this several times and compare with a random learning strategy\n",
    "(obtainedRewardsOptimal,env1)=Q1.simulateLearnedStrategy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
